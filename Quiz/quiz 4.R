#8
#1. wrong answer the model generated by code does not match your answer -1
#2. no diagnostics -1

setwd("~/Desktop")
data <- read.csv2("math.csv",header = TRUE)
attach(data)
library("dplyr")
data %>% mutate((G1+G2+G3)/3)
data <- data %>% mutate(G=(G1+G2+G3)/3)
dataG  <- subset(data, select = -c(G1,G2,G3))
fit1 <- lm(G ~ (.) , data = dataG)
'''
Residual standard error: 2.09e-13 on 355 degrees of freedom
Multiple R-squared:    0.5,	Adjusted R-squared:  0.4451 
F-statistic: 9.104 on 39 and 355 DF,  p-value: < 2.2e-16
'''
summary(fit1)
step <- step(fit1)
step$call
model <- lm(formula = G ~ age + Pstatus + Medu + Mjob + Fjob + schoolsup + 
            famsup, data = dataG)
summary(model)

'''
If we use stepwise regression, 
the final model would be (formula = G ~ age + Pstatus + Medu + Mjob + Fjob + schoolsup + 
    famsup, data = dataG)
and the goodness-of-fit indexes could be adjusted Rˆ2 0.4829 , and AIC -23067.32

'''
par(mfrow = c(2,2)) 
plot(model)
'''
Scale-Location plot shows that we have a heteroscedasticity problem.

'''

influence.measures(model)
'''
Use r built-in method to identify influential points,
1     *
11    *
22    *
25    *
33    *
39    *
45    *
'''
#Q2
library(leaps)
models <- regsubsets(G~., data = dataG, nvmax = 8)
res.sum <- summary(models)
data.frame( Adj.R2 = which.max(res.sum$adjr2), CP = which.min(res.sum$cp), BIC = which.min(res.sum$bic) )

'''
Our “best” models depending on which metrics we consider. Here we choose BIC and cp values and R squared
We choose our eightth model as our best model
namely these variables:
sexM  Medu Mjobhealth Mjobservices Fjobteacher failures schoolsupyes goout
'''
coef(models, 8)
'''
(Intercept)         sexM   Mjobhealth Mjobservices  Fjobteacher    studytime     failures 
  10.5205927    1.0635661    2.0137184    1.3153730    1.5615573    0.5476664   -1.7232660 
schoolsupyes        goout 
  -1.4542200   -0.4267998 
'''
dataG$Mjob <- as.numeric(data$Mjob) - 1
dataG$Fjob <- as.numeric(data$Fjob) - 1
dataG$sup <- as.numeric(data$schoolsup) -1
plot(models,8)
fit2 <- lm(G ~sex+Mjob+Fjob+studytime+failures+schoolsup+goout,data=dataG)
summary(fit2)
'''
The Adjusted R-squared:  0.186 and pvalue is  4.849e-16
'''
influence.measures(fit2)
'''
Influential points include
64,59,19,3
'''
#Q3
'''
The potential problems would be
(1) There may be a  heteroscedasticity problem.
(2) We assume linearyship between variables while it could exist non-linearyship between variables and we may have to consider transformation
(3) The regressors may not be random 
'''
