test <- data[-split,]
set.seed(123)
model <- train(
Survived ~., data = train, method = "rf",
trControl = trainControl("cv", number = 10),
importance = TRUE
)
model$finalModel
confusionMatrix(model$finalModel$predicted,train$Survived,positive = "1")
data <- read.csv('~/Desktop/data/Titanic.csv')
data <- subset(data, select = -c(Name,Ticket,Cabin))
data <- subset(data, is.na(Age) == FALSE)
data$Survived <- as.factor(data$Survived)
nrow(data)
set.seed(123)
split <- data$Survived %>%
createDataPartition(p = 0.75, list = FALSE)
train <- data[split,]
test <- data[-split,]
set.seed(123)
model <- train(
Survived ~., data = train, method = "rf",
trControl = trainControl("cv", number = 10),
importance = TRUE
)
# Best tuning parameter
model$bestTune
model$finalModel
confusionMatrix(model$finalModel$predicted,train$Survived,positive = "1")
confusionMatrix(model$finalModel$predicted,train$Survived,positive = "0")
model$finalModel
confusionMatrix(model$finalModel$predicted,train$Survived,positive = "1")
model$finalModel
model$finalModel$predicted
model$finalModel$predicted
train$Survived,positive
train$Survived
confusionMatrix(model$finalModel$predicted,train$Survived,positive = "1")
59/(59+13)
95/(95+11)
model$finalModel
148/(70+148)
confusionMatrix(model$finalModel$predicted,train$Survived)
model$finalModel
confusionMatrix(model$finalModel$predicted,train$Survived,positive = "1")
table(test$Survived,pred)
pred <- model %>% predict(test)
table(test$Survived,pred)
table(test$Survived,pred)
confusionMatrix(pred,test$Survived,positive = "1")
table(test$Survived,pred)
pred <- model %>% predict(test)
table(test$Survived,pred)
#confusionMatrix(pred,test$Survived,positive = "1")
***das ***
varImp(model)
varImp(fit, type = 1 )
varImp(model)
varImp(fit, type = 1 )
set.seed(123)
model <- train(
SalePrice ~., data = train.data, method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
model$bestTune
coef(model$finalModel, model$bestTune$lambda)
x.test <- subset(test.data, select = -SalePrice)
predictions <- model %>% predict(x.test)
data.frame(
RMSE = RMSE(predictions, test.data$SalePrice),
Rsquare = R2(predictions, test.data$SalePrice)
)
x.test <- subset(test.data)
predictions <- model %>% predict(x.test)
#x.test <- model.matrix(SalePrice ~.,data = test.data)[,-1]
#net.model <- glmnet(x, y, alpha = 0.1, lambda = net$bestTune$lambda)
#predictions <- net.model %>% predict(x.test)
data.frame(
RMSE = RMSE(predictions, test.data$SalePrice),
Rsquare = R2(predictions, test.data$SalePrice)
)
coef(model$finalModel, model$bestTune$lambda)
x.test <- subset(test.data)
predictions <- model %>% predict(x.test)
knitr::opts_chunk$set(echo = TRUE)
x.test <- model.matrix(medv ~., test.data)[,-1]
library(MASS)
library(tidyverse)
library(caret)
library(glmnet)
data("Boston", package = "MASS")
set.seed(123)
training.samples <- Boston$medv %>% createDataPartition(p = 0.75, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
x.test <- model.matrix(medv ~., test.data)[,-1]
predictions <- model %>% predict(x.test)
data("Boston", package = "MASS")
set.seed(123)
training.samples <- Boston$medv %>% createDataPartition(p = 0.75, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
x <- model.matrix(medv~., train.data)[,-1]
y <- train.data$medv
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0)
cv$lambda.min
x.test <- model.matrix(medv ~., test.data)[,-1]
predictions <- model %>% predict(x.test)
x.test <- model.matrix(medv ~., test.data)[,-1]
predictions <- model %>% predict(x.test)
x.test <- test.data
predictions <- model %>% predict(x.test)
x.test <- subset(test.data,select=-c(LotArea))
x.test <- subset(test.data,select=-c(LotArea))
data("Boston", package = "MASS")
set.seed(123)
training.samples <- Boston$medv %>% createDataPartition(p = 0.75, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
x <- model.matrix(medv~., train.data)[,-1]
y <- train.data$medv
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0)
cv$lambda.min
set.seed(123)
model <- train(
medv ~., data = train.data, method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
model$bestTune
x.test <- model.matrix(medv ~., test.data)[,-1]
predictions <- model %>% predict(x.test)
data.frame(
RMSE = RMSE(predictions, test.data$medv),
Rsquare = R2(predictions, test.data$medv)
)
predictions <- model %>% predict(x.test)
install.packages("ROCR")
Data <- read.csv('~/Desktop/data/caesarian.csv')
Data$Caesarian <- as.factor(Data$Caesarian)
confusionMatrix(predicted.classes,test.data$Caesarian,positive = "1")
confusionMatrix(predicted.classes,test.data$Caesarian,positive = "1")
model1 <- rpart(Caesarian ~., data = train.data, method = "class", control = list(cp = 0, maxcompete = 0, maxsurrogate = 0, xval = 0, minsplit = 10, minbucket = 5))
library(rpart)
library(rattle)
model1 <- rpart(Caesarian ~., data = train.data, method = "class", control = list(cp = 0, maxcompete = 0, maxsurrogate = 0, xval = 0, minsplit = 10, minbucket = 5))
9/(3+9)
library(caret)
model2 <- train(
Caesarian ~., data = Train, method = "rpart",
trControl = trainControl("cv", number = 10),
tuneLength = 100
)
Data <- read.csv('~/Desktop/data/caesarian.csv')
Data$Caesarian <- as.factor(Data$Caesarian)
library(caTools)
set.seed(123)
split <- sample.split(Data$Caesarian, SplitRatio = 0.75)
Train <- subset(Data, split == TRUE)
Test <- subset(Data, split == FALSE)
library(rpart)
library(rattle)
model <- rpart(Caesarian~., data = Train, control = rpart.control(cp=0))
fancyRpartPlot(model)
pred <- predict(model, newdata = Test, type = 'class')
table(Test$Caesarian,pred)
library(caret)
model2 <- train(
Caesarian ~., data = Train, method = "rpart",
trControl = trainControl("cv", number = 10),
tuneLength = 100
)
plot(model2)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop")
library('MASS')
data <- read.table("banknote.txt",header=FALSE,sep=',')
setwd("~/Desktop/data")
library('MASS')
data <- read.table("banknote.txt",header=FALSE,sep=',')
head(data)
colnames(data) <- c('variance','skewness','curtosis','entropy','class')
install.packages("caTools")
set.seed(123)
library(caTools)
library('dplyr')
library('caret')
training.samples <- data$class %>%
createDataPartition(p = 0.75, list = FALSE) #Caret
Train <- data[training.samples, ]
Test <- data[-training.samples, ]
install.packages("caTools")
install.packages("caTools")
set.seed(123)
library(caTools)
library('dplyr')
library('caret')
training.samples <- data$class %>%
createDataPartition(p = 0.75, list = FALSE) #Caret
Train <- data[training.samples, ]
Test <- data[-training.samples, ]
knitr::opts_chunk$set(echo = TRUE)
library('MASS')
full2 <- glm(class ~ .^2, family = 'binomial' , control = list(maxit =50), data = Train)
summary(full2)
suppressWarnings(
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE)
)
summary(step_model2)
BIC(step_model2 )
BIC(glm(class ~ variance + skewness + curtosis + variance:curtosis, family = "binomial", data = Train))
BIC(step_model2)
summary(step_model2)
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE)
)
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE)
suppressWarnings(
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE)
)
summary(step_model2)
summary(step_model2)
BIC(step_model2)
suppressWarnings(
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE)
)
summary(step_model2)
summary(step_model2)
BIC(step_model2)
knitr::opts_chunk$set(echo = TRUE)
Train <- subset(bwt, split == TRUE)
Test <- subset(bwt, split == FALSE)
fit1 <- glm(low ~ (.)^2 + I(scale(age)^2) + I(scale(lwt)^2), family = 'binomial', data = Train)
suppressWarnings(fit2 <- stepAIC(fit1,  scope = list(lower=low~1,upper=formula(fit1)), k = log(nrow(Train)), trace = FALSE))
summary(fit2)
BIC(fit2)
library('MASS')
full2 <- glm(class ~ .^2, family = 'binomial' , control = list(maxit =50), data = Train)
data <- read.table("banknote.txt",header=FALSE,sep=',')
setwd("/Desktop/data")
setwd("\Desktop\data")
setwd("~/Desktop/data")
setwd("~/Desktop/data")
data <- read.table("banknote.txt",header=FALSE,sep=',')
head(data)
colnames(data) <- c('variance','skewness','curtosis','entropy','class')
install.packages("caTools")
set.seed(123)
library(caTools)
library('dplyr')
library('caret')
training.samples <- data$class %>%
createDataPartition(p = 0.75, list = FALSE) #Caret
Train  <- data[training.samples, ]
Test <- data[-training.samples, ]
install.packages("caTools")
library('MASS')
full2 <- glm(class ~ .^2, family = 'binomial' , control = list(maxit =50), data = Train)
summary(full2)
suppressWarnings(
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE)
)
summary(step_model2)
BIC(step_model2)
# ?? quiz is right, but I think it is wrong:  BIC(glm(class ~ variance + skewness + curtosis + variance:curtosis, family = "binomial", data = Train))
summary(step_model2)
BIC(step_model2)
knitr::opts_chunk$set(echo = TRUE)
summary(step_model2)
probabilities <- step_model2 %>% predict(Train, type = "response")
train.control <- trainControl(method = "LOOCV")
library('dplyr')
library('caret')
train.control <- trainControl(method = "LOOCV")
suppressWarnings(
cv.result <- train(class ~ variance + skewness + curtosis + variance:curtosis, data = data, method = "glm", family = 'binomial', trControl = train.control)
)
predict.cv.prob <- cv.result %>% predict(data)
predict.cv.classes <- ifelse(predict.cv.prob>0.5,1,0)
test.cv.classes <-  data$class
confusionMatrix(as.factor(predict.cv.classes),  as.factor(test.cv.classes),
positive = "1")
#table(data$class, round(cv.result$pred$pred))
probabilities2 <- step_model2 %>% predict(Test, type = "response")
predicted.classes2 <- ifelse(probabilities2 > 0.5, 1, 0)
train.classes2 <-  Test$class
# mean(predicted.classes == train.data$low)
#table(train.classes ,predicted.classes)
confusionMatrix(as.factor(predicted.classes2),  as.factor(train.classes2),
positive = "1")
table(bwt$low)
fit1 <- glm(low ~ (.)^2 + I(scale(age)^2) + I(scale(lwt)^2), family = 'binomial', data = Train)
library(MASS)
data("birthwt", package = "MASS")
bwt <- with(birthwt, {
race <- factor(race, labels = c("white", "black", "other"))
ptd <- factor(ptl > 0)
ftv <- factor(ftv)
levels(ftv)[-(1:2)] <- "2+"
data.frame(low = factor(low), age, lwt, race, smoke = (smoke > 0), ptd, ht = (ht > 0), ui = (ui > 0), ftv)
})
table(bwt$low)
library(caTools)
set.seed(123)
split = sample.split(bwt$low, SplitRatio = 0.75)
Train <- subset(bwt, split == TRUE)
Test <- subset(bwt, split == FALSE)
fit1 <- glm(low ~ (.)^2 + I(scale(age)^2) + I(scale(lwt)^2), family = 'binomial', data = Train)
suppressWarnings(fit2 <- stepAIC(fit1,  scope = list(lower=low~1,upper=formula(fit1)), k = log(nrow(Train)), trace = FALSE))
summary(fit2)
BIC(fit2)
summary(fit2)
fit1 <- glm(low ~ (.)^2 + I(scale(age)^2) + I(scale(lwt)^2), family = 'binomial', data = Train)
suppressWarnings(fit2 <- stepAIC(fit1,  scope = list(lower=low~1,upper=formula(fit1)), k = log(nrow(Train)), trace = FALSE))
summary(fit2)
library(MASS)
data("birthwt", package = "MASS")
bwt <- with(birthwt, {
race <- factor(race, labels = c("white", "black", "other"))
ptd <- factor(ptl > 0)
ftv <- factor(ftv)
levels(ftv)[-(1:2)] <- "2+"
data.frame(low = factor(low), age, lwt, race, smoke = (smoke > 0), ptd, ht = (ht > 0), ui = (ui > 0), ftv)
})
table(bwt$low)
library(caTools)
set.seed(123)
split = sample.split(bwt$low, SplitRatio = 0.75)
Train <- subset(bwt, split == TRUE)
Test <- subset(bwt, split == FALSE)
fit1 <- glm(low ~ (.)^2 + I(scale(age)^2) + I(scale(lwt)^2), family = 'binomial', data = Train)
suppressWarnings(fit2 <- stepAIC(fit1,  scope = list(lower=low~1,upper=formula(fit1)), k = log(nrow(Train)), trace = FALSE))
summary(fit2)
BIC(fit2)
predictTrain <- predict(fit2, type="response")
#confusion table
table(Train$low, predictTrain > 0.5)
knitr::opts_chunk$set(echo = TRUE)
BIC(fit2)
BIC(fit2)
summary(fit2)
fit1 <- glm(low ~ (.)^2 + I(scale(age)^2) + I(scale(lwt)^2), family = 'binomial', data = Train)
suppressWarnings(fit2 <- stepAIC(fit1,  scope = list(lower=low~1,upper=formula(fit1)), k = log(nrow(Train)), trace = FALSE))
summary(fit2)
fit1 <- glm(low ~ (.)^2 + I(scale(age)^2) + I(scale(lwt)^2), family = 'binomial', data = Train)
fit1
confusionMatrix(predictTrain > 0.5,Train$low)
predict.cv.classes <- ifelse(predictTrain>0.5,1,0)
test.cv.classes <-  data$class
confusionMatrix(as.factor(predict.cv.classes),  as.factor(test.cv.classes),
positive = "1")
predict.cv.classes <- ifelse(predictTrain>0.5,1,0)
test.cv.classes <-  Train$low
confusionMatrix(as.factor(predict.cv.classes),  as.factor(test.cv.classes),
positive = "1")
#confusion table
table(Train$low, predictTrain > 0.5)
predict.cv.classes <- ifelse(predictTrain>0.5,1,0)
test.cv.classes <-  Train$low
confusionMatrix(as.factor(predict.cv.classes),  as.factor(test.cv.classes),
positive = "1")
#confusion table
table(Train$low, predictTrain > 0.5)
ROCRpred <- prediction(predictTest, Test$low)
library(ROCR)
ROCRpred <- prediction(predictTest, Test$low)
predictTest <- predict(fit2, newdata = Test, type = "response")
#confusion table
table(Test$low, predictTest > 0.5)
library(ROCR)
ROCRpred <- prediction(predictTest, Test$low)
ROCRpred <- prediction(predictTest, Test$low)
ROCRperf <- performance(ROCRpred, "tpr", "fpr")
plot(ROCRperf)
performance(ROCRpred, 'auc')
performance(ROCRpred, 'auc')
performance(ROCRpred, 'auc')
performance(ROCRpred, 'auc')
log(nrow(Train)), trace = FALSE))
summary(fit2)
summary(fit2)
coef(fit2)
knitr::opts_chunk$set(echo = TRUE)
step_model1 <- stepAIC(full,trace = TRUE,k = log(nrow(data)))
knitr::opts_chunk$set(echo = TRUE)
step_model1 <- stepAIC(full,trace = TRUE,k = log(nrow(data)))
??stepAIC
fit1 <- glm(low ~ (.)^2 + I(scale(age)^2) + I(scale(lwt)^2), family = 'binomial', data = Train)
suppressWarnings(fit2 <- stepAIC(fit1, keep=TRUE, scope = list(lower=low~1,upper=formula(fit1)), k = log(nrow(Train)), trace = FALSE))
suppressWarnings(fit2 <- stepAIC(fit1, keep=AIC, scope = list(lower=low~1,upper=formula(fit1)), k = log(nrow(Train)), trace = FALSE))
summary(fit2)
BIC(fit2)
summary(fit2)
full2 <- glm(class ~ (.)^2, family = 'binomial' , control = list(maxit =50), data = data)
summary(full2)
suppressWarnings(
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE)
)
summary(step_model2)
full2 <- glm(class ~ (.)^2, family = 'binomial' , control = list(maxit =50), data = data)
summary(full2)
suppressWarnings(
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE,direction = backward)
)
full2 <- glm(class ~ (.)^2, family = 'binomial' , control = list(maxit =50), data = data)
summary(full2)
suppressWarnings(
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE,direction = "backward")
)
full2 <- glm(class ~ (.)^2, family = 'binomial' , control = list(maxit =50), data = data)
summary(full2)
suppressWarnings(
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE)
)
summary(step_model2)
BIC(glm(class ~ variance + skewness + curtosis + variance:curtosis,
family = "binomial", data = data))
full2 <- glm(class ~ (.)^2, family = 'binomial' , control = list(maxit =50), data = data)
summary(full2)
suppressWarnings(
step_model2 <- stepAIC(full2, scope = list(lower=~1,upper=full2),k = log(nrow(data)), trace = FALSE)
)
)
summary(step_model2)
BIC(glm(class ~ variance + skewness + curtosis + variance:curtosis,
family = "binomial", data = data))
BIC(step_model2)
BIC(glm(class ~ variance + skewness + curtosis + variance:curtosis,
family = "binomial", data = data))
library(leaps)
library(bestglm)
library(dummies)
best_model3 <- bestglm(data, family = binomial, IC = 'BIC')
best_model3
BIC(glm(class ~ variance + skewness + curtosis , data = data, family = 'binomial'))
?bestglm
varImp(fit, type = 1 )
varImp(model)
fit <- randomForest(Survived ~ ., data = train, importance = TRUE)
library(tidyverse)
library(caret)
library(randomForest)
data <- read.csv('~/Desktop/Titanic.csv')
data <- read.csv('~/Desktop/data/Titanic.csv')
data <- subset(data, select = -c(Name,Ticket,Cabin))
data <- subset(data, is.na(Age) == FALSE)
data$Survived <- as.factor(data$Survived)
nrow(data)
set.seed(123)
split <- data$Survived %>%
createDataPartition(p = 0.75, list = FALSE)
train <- data[split,]
test <- data[-split,]
set.seed(123)
model <- train(
Survived ~., data = train, method = "rf",
trControl = trainControl("cv", number = 10),
importance = TRUE
)
# Best tuning parameter
model$bestTune
fit <- randomForest(Survived ~ ., data = train, importance = TRUE)
fit
varImp(fit, type = 1 )
importance(fit, type = 1)
varImp(fit, type = 1 )
importance(fit, type = 1)
best_model3
data
View(race)
View(model2)
fit1_step <- step(fit1, k = 2)
data <- read.csv('~/Desktop/data/math.csv',header = T,sep = ';')
fit1 <- glm((G1+G2+G3)/3 ~ . ,data= data)
fit1_step <- step(fit1, k = 2)
fit1_step <- step(fit1, k = 2)
summary(fit1_step)
summary(fit1_step)
data <- read.csv('~/Desktop/data/math.csv',header = T,sep = ';')
fit1 <- glm((G1+G2+G3)/3 ~ . ,data= data)
fit1_step <- step(fit1, k = 2)
summary(fit1_step)
fit2 <- glm((G1+G2+G3)/3 ~(sex + studytime + failures + schoolsup + famsup + higher + romantic + goout + health)^2, data = data )
summary(fit2_step)
fit2 <- glm((G1+G2+G3)/3 ~(sex + studytime + failures + schoolsup + famsup + higher + romantic + goout + health)^2, data = data )
fit2_step <- step(fit2, k = 2)
summary(fit2_step)
knitr::opts_chunk$set(echo = TRUE)
summary(fit2_step)
fit3 <- glm(formula = (G1 + G2 + G3)/3 ~ sex + studytime + failures +
schoolsup + famsup + higher + romantic + goout + health +
sex:failures + sex:schoolsup + sex:famsup + studytime:schoolsup +
studytime:famsup + studytime:goout + failures:schoolsup +
failures:higher + failures:goout + schoolsup:health + higher:goout,
data = data)
summary(fit3)
View(ftv)
summary(fit4)$bic
library(leaps)
fit4 <- regsubsets((G1+G2+G3)/3 ~ ., data = data ,nvmax = 39, nbest = 1, really.big = T)
summary(fit5)$bic
summary(fit4)$bic
which(summary(fit4)$bic == min(summary(fit4)$bic))
fit4 <- regsubsets((G1+G2+G3)/3 ~ ., data = data ,nvmax = 39, nbest = 1, really.big = T)
View(fit4)
data$Gender <- as.numeric(data$Gender) - 1
data <- read.table('https://www.randomservices.org/random/data/Galton.txt',header = T)
data <- subset(data, select = -c(Family))
data$Gender <- as.numeric(data$Gender) - 1
data$Gender
